{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Deep Q-Learning \n",
    "---\n",
    "In this notebook, you will implement a DQN agent with OpenAI Gym's [Cart Pole](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) - environment.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PrzemekSekula/ReinforcementLearningClasses/blob/master/DQLearning/00_VanillaDQL.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "\n",
    "\n",
    "This is a relatively simple version of the algorithm, it differs from classical Q-Learning in four main ways:\n",
    "- we are using a neural network to estimate the state-action value\n",
    "- the neural network estimates all state-action values for a given state\n",
    "- we are using Replay Buffer to store the `s, a, r, s', done` tuples, and learning from the data sampled from the Replay Buffer, not from immediate experience.\n",
    "- we are using the decaying exploration rate $\\epsilon$. In the beginning of learning the agent is exploring a lot, and the exploration rate is reduced in time.\n",
    "\n",
    "Your task is to complete the code in this notebook. The places were you should create your own implementation are described in this notebook and marked with `# ENTER YOUR CODE HERE` comments.\n",
    "\n",
    "*Note: This code uses replay buffer implemented in the `reply_buffer.py` file. This implementation comes from [OpenAI baselines GitHub repository](https://github.com/openai/baselines/tree/master/baselines/deepq).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install --upgrade gym\n",
    "    !wget https://raw.githubusercontent.com/PrzemekSekula/ReinforcementLearningClasses/main/DQLearning/replay_buffer.py\n",
    "    !wget https://raw.githubusercontent.com/PrzemekSekula/ReinforcementLearningClasses/main/DQLearning/segment_tree.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print ('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Implement the neural network\n",
    "Implement the neural network (`QNetwork` class):\n",
    "1) In `__init__()` Create a neural network\n",
    "2) In `forward()` implement the forward pass with ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Fully connected (dense) neural network that estimates Q values.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Args:\n",
    "            state_size (int): Dimension of each state. It equals\n",
    "                the number of features in the network.\n",
    "            action_size (int): Dimension of each action. It equals \n",
    "                the number of the network outputs\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = None # ENTER YOUR CODE HERE\n",
    "        self.fc2 = None # ENTER YOUR CODE HERE\n",
    "        self.fc3 = None # ENTER YOUR CODE HERE\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\n",
    "        Args:\n",
    "            state (torch.Tensor): The state of the environment\n",
    "        Returns:\n",
    "            torch.Tensor: The action values\n",
    "        \"\"\"\n",
    "        # ENTER YOUR CODE HERE\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Implement the Agent:\n",
    "Implement the `Agent` class.\n",
    "1) In `__init__()` create a neural network, optimizer and Replay Buffer\n",
    "2) In `step()` add the experience to the memory buffer\n",
    "3) In `learn()` calculate `Q_targets` and `loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size,\n",
    "                 buffer_size = int(1e5),\n",
    "                 batch_size = 64,\n",
    "                 gamma = 0.99,\n",
    "                 lr = 5e-4,\n",
    "                 update_every = 4,\n",
    "                 device = None):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        Args:\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): replay buffer size. Default: int(1e5)\n",
    "            batch_size (int): minibatch size. Default: 64\n",
    "            gamma (float): discount factor. Default: 0.99\n",
    "            lr (float): learning rate. Default: 5e-4\n",
    "            update_every (int): how often to update the network. Default: 1        \n",
    "            device (torch.device): device to use. Default: None    \n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.update_every = update_every\n",
    "        self.device = device\n",
    "\n",
    "        # Q-Network\n",
    "        fc1, fc2 = 64, 16 # Size of the layers\n",
    "        # TODO: Create a network with 2 hidden layers (fc1 and fc2 nodes)\n",
    "        self.qnetwork = None # ENTER YOUR CODE HERE\n",
    "        \n",
    "        # TODO: Create the optimizer (Adam with learning rate lr)\n",
    "        self.optimizer = None # ENTER YOUR CODE HERE\n",
    "\n",
    "        # TODO: Create a Replay Buffer. Use ReplayBuffer class from the \n",
    "        # reply_buffer.py file. Use buffer_size as the buffer size\n",
    "        self.memory = None # ENTER YOUR CODE HERE\n",
    "        \n",
    "        # Initialize time step (for updating every self.update_every steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # TODO: Save experience in replay buffer. Use self.memory.add()\n",
    "        # ENTER YOUR CODE HERE\n",
    "        \n",
    "        # Learn every self.update_every time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample(self.batch_size)\n",
    "                self.learn(experiences)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork(state)\n",
    "        self.qnetwork.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        device = self.device  # for shortening the following lines\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.LongTensor(actions.reshape(-1, 1)).to(device)\n",
    "        rewards = torch.FloatTensor(rewards.reshape(-1, 1)).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones = torch.FloatTensor(dones.reshape(-1, 1)).to(device)        \n",
    "        \n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_next = self.qnetwork(next_states).detach().max(dim=1, keepdim=True)[0]\n",
    "        # TODO: Compute Q targets for current states \n",
    "        Q_targets = None # ENTER YOUR CODE HERE\n",
    "\n",
    "        # Get expected Q values\n",
    "        Q_curr = self.qnetwork(states).gather(1, actions)\n",
    "\n",
    "        # TODO: Calculate loss. Use F.mse_loss\n",
    "        loss = None # ENTER YOUR CODE HERE\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def save(self, path = 'checkpoint.pth'):\n",
    "        \"\"\"Saves the network\n",
    "        Args:\n",
    "            path (str): path to save the network. Default is 'checkpoint.pth'\n",
    "        \"\"\"\n",
    "        torch.save(self.qnetwork.state_dict(), path)\n",
    " \n",
    " \n",
    "    def load(self, path = 'checkpoint.pth'):\n",
    "        \"\"\"Loads the network\n",
    "        Args:\n",
    "            path (str): path with the weights. Default is 'checkpoint.pth'\n",
    "        \"\"\"\n",
    "        self.qnetwork.load_state_dict(torch.load(path))\n",
    "        \n",
    "agent = Agent(\n",
    "    state_size=env.observation_space.shape[0], \n",
    "    action_size=env.action_space.n, device = device)        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Train the Agent\n",
    "Complete the `train_agent()` function.\n",
    "1) Select and perform an action\n",
    "2) Use `agent.step()` to update the agent's knowledge\n",
    "3) Decay the explorarion rate epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(\n",
    "        n_episodes = 3000,    # Maximum number of episodes\n",
    "        max_t = 1000,         # Maximum number of timesteps per episode\n",
    "        eps_start = 1.0,      # Starting value of epsilon\n",
    "        eps_end = 0.01,       # Minimum value of epsilon\n",
    "        eps_decay = 0.995,    # Decay rate of epsilon\n",
    "        expected_reward = 230 # Expected average reward to solve the environment\n",
    "\n",
    "        ):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    Args:\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for \n",
    "            epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): decay factor (per episode) \n",
    "            for decreasing epsilon\n",
    "        expected_reward (float): finish when the average score\n",
    "            is greater than this value\n",
    "    Returns:\n",
    "        scores (list): list of scores from each episode\n",
    "    \"\"\"\n",
    "    scores = []                        \n",
    "    scores_window = deque(maxlen=100)  \n",
    "    eps = eps_start                    \n",
    "    for episode in range(1, n_episodes+1):\n",
    "        state, info = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            # TO DO: select an action\n",
    "            action = None # ENTER YOUR CODE HERE\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            # TO DO: use agent.step() to update the agent\n",
    "            # ENTER YOUR CODE HERE\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       \n",
    "        scores.append(score)\n",
    "        \n",
    "        # TO DO: Decay epsilon by multiplying it by eps_decay             \n",
    "        eps = None # ENTER YOUR CODE HERE \n",
    "        \n",
    "        mean_score = np.mean(scores_window)\n",
    "        \n",
    "        print(f'\\rEpisode {episode}\\tAverage Score: {mean_score:.2f}     ', end=\"\")\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            \n",
    "            print(f'\\rEpisode {episode}\\tAverage Score: {mean_score:.2f}')\n",
    "            agent.save('checkpoint.pth')\n",
    "        if mean_score >= expected_reward:\n",
    "            print(f'\\nDone in {episode:d} episodes!\\tAverage Score: {mean_score:.2f}')\n",
    "            agent.save('checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = train_agent()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the agent's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import imageio\n",
    "\n",
    "def create_video(agent, video_path):\n",
    "    env = gym.make('CartPole-v1', render_mode = 'rgb_array')\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    img_list = [env.render()]\n",
    "\n",
    "    for t in range(1000):\n",
    "        action = agent.act(state, 0)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        img_list.append(env.render())\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "    env.close()\n",
    "\n",
    "    # Create a video file using imageio\n",
    "    imageio.mimsave(video_path, img_list, fps=30)\n",
    "\n",
    "    print (f'Visualization done. Episode ended after {t} steps.')\n",
    "    \n",
    "    return img_list\n",
    "\n",
    "video_path = 'output.mp4'\n",
    "create_video(agent, video_path)\n",
    "\n",
    "mp4 = open(video_path,'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=400 controls>\n",
    "    <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
