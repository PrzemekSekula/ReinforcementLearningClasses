{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Q-Learning, Duelling Q-Learning, Learning from images\n",
    "---\n",
    "In this notebook, you will implement a DQN agent with OpenAI Gym's [Pong](https://www.gymlibrary.dev/environments/atari/pong/) environment.\n",
    "\n",
    "Comparing to the previous notebooks, the following enhancments/changes are introduced.\n",
    "- Agent is learning from images (pixels), not from vector-based states. Thus, a Convolutional Neural Network is used.\n",
    "- Double DQN algorithm is used\n",
    "- Duelling Neural Network is used.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PrzemekSekula/ReinforcementLearningClasses/blob/master/DQLearning/02_CNN_DuelingQN.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "\n",
    "### Double Deep Q-Learning explained \n",
    "---\n",
    "In the previous notebook we were using additional neural network `target neural network` to estimate $TD_{target}$ according to the formula: \n",
    "$$TD_{target} = R + \\gamma \\max\\limits_{a'} Q(S_{t+1}, \\argmax\\limits_{a'}Q(S_{t+1}, a', w^-), w^-)$$ \n",
    "In other words, we were using target neural network to select the best action $a'$, and then we were using the same network to estimate the value of this action. Such an approach can be prone to overestimation. \n",
    "To solve this problem, we may select the best action using other set of weights, and then estimate the value of this action with target neural network. In fact, we already have additonal set of weights $w$ that comes from the `local neural network`, so our $TD_{target}$ estimations can be performed as follows:\n",
    "$$TD_{target} = R + \\gamma \\max\\limits_{a'} Q(S_{t+1}, \\argmax\\limits_{a'}Q(S_{t+1}, a', w^-), w)$$\n",
    "\n",
    "### Dueling Deep Q-Network Explained\n",
    "---\n",
    "\n",
    "Dueling Q network architecture was proposed in the [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581) paper. The general idea is to calculate state action values using state values $V(s)$ and advantages $A(s, a) = Q(s, a) - V(s)$. It is being done by bulding a network architecture with parallel layers for calculating $V(s)$ and  $A(s, a)$ as presented in the image (image from the abovementioned paper).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/PrzemekSekula/ReinforcementLearningClasses/main/img/DuelingQNetwork.jpg\" alt=\"Dueling Q Network\" style=\"width: 640px;\">\n",
    "\n",
    "The only issue is how to ensure that $V(s)$ will produce a reasonable estimate for state values. We are doing it, by assuming, that the mean advantage value should always be 0. Thus the final formula:\n",
    "\n",
    "$$Q(s, a) = V(s) + A(s, a) - \\frac{1}{|A|}\\sum\\limits_{a'}A(s, a')$$\n",
    "\n",
    "This is all implemented in `DuelingQNetwork` class.\n",
    "\n",
    "There is one more improvement - to keep it consistent with the paper I clipped the gradients to 10 in order to avoid gradient exploding problem.\n",
    "\n",
    "\n",
    "### TO DO\n",
    "---\n",
    "\n",
    "Your task is to complete the code in this notebook. The places were you should create your own implementation are described in this notebook and marked with `# ENTER YOUR CODE HERE` comments.\n",
    "\n",
    "*Note: This code uses replay buffer implemented in the `reply_buffer.py` file. This implementation comes from [OpenAI baselines GitHub repository](https://github.com/openai/baselines/tree/master/baselines/deepq).*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install --upgrade gym\n",
    "    !pip install --upgrade gym[atari]\n",
    "    !pip install --upgrade gym[accept-rom-license]\n",
    "\n",
    "    !wget https://raw.githubusercontent.com/PrzemekSekula/ReinforcementLearningClasses/main/DQLearning/replay_buffer.py\n",
    "    !wget https://raw.githubusercontent.com/PrzemekSekula/ReinforcementLearningClasses/main/DQLearning/segment_tree.py\n",
    "    !wget https://raw.githubusercontent.com/PrzemekSekula/ReinforcementLearningClasses/main/DQLearning/Notebook02_steps300.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print ('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "state, info = env.reset()\n",
    "print('State shape:', state.shape)\n",
    "print ('Valid actions:', env.unwrapped.get_action_meanings())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "This environment has six actions. To simplify things, we are going to use only three of them:\n",
    "- `FIRE` (1) - do not move\n",
    "- `RIGHTFIRE` (4) - move right\n",
    "- `LEFTFIRE` (5) - move left\n",
    "\n",
    "Pressing `FIRE` is important only in the beginning - it starts a game. In any other situation there is no difference between `NOOP` and `FIRE`, `RIGHT` and `RIGHTFIRE` etc. By adding fire to every action we force the agent to start a game, when it is necessary.\n",
    "\n",
    "With three actions our neural network should have three output neurons. The only problem is with numbering actions. From the neural network perspective, the most natural numeration is 0, 1, and 2. This way we can use `argmax` or `index` to select particular action. Environment uses 1, 4, and 5, so we need a function that maps NN-related action numbers to Environment-related action numbers. We called this function `preprocess_action`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "STATE_SIZE = (2, 48, 48) # 2 frames of 48x48 pixels (gray scale)\n",
    "ACTION_SIZE = 3 # 3 possible actions: 0 (FIRE), 1 (RIGHTFIRE), 2 (LEFTFIRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_action(action):\n",
    "    \"\"\"Changes the action from a neural network into the real action.\n",
    "    We are using it, because we want to use only three actions:\n",
    "    FIRE (1), RIGHTFIRE (4), and LEFTFIRE (5).\n",
    "    Args:\n",
    "        action (int): number of neuron (0 , 1 or 2)\n",
    "\n",
    "    Returns:\n",
    "        int: number of action (4 or 5)\n",
    "    \"\"\"\n",
    "    if action == 0: \n",
    "        return 1 # FIRE\n",
    "    elif action == 1:\n",
    "        return 4 # RIGHTFIRE\n",
    "    elif action == 2:\n",
    "        return 5 # LEFTFIRE\n",
    "    else:\n",
    "        raise ValueError('Action must be 0, 1 or 2')  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image preprocessing\n",
    "To simplify things the image is preprocessed\n",
    "- The upper and lower (irrelevant) parts are cropped\n",
    "- Image is changed into grayscale (we use only last channel)\n",
    "- Image is resized (by default to 48x48)\n",
    "\n",
    "The original and preprocessed images are visualized in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocess(image, output_size = STATE_SIZE[1:]):\n",
    "    \"\"\"\n",
    "    Image preprocessing function.\n",
    "    - Crops the image and leaves only the playing area\n",
    "    - Changes the RGB image to gray scale\n",
    "    - resizes the image to the output_size\n",
    "\n",
    "    Args:\n",
    "        image (np.array): input image\n",
    "        output_size ((int, int)): Size of the output image. Defaults to STATE_SIZE[1:].\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    img = np.mean(image[34:-16:2,::2], axis=-1)/255.\n",
    "    \n",
    "    return resize(img, output_size, anti_aliasing=True)\n",
    "\n",
    "# Test the image_preprocess function\n",
    "state, _ = env.reset()\n",
    "for _ in range(25):\n",
    "    state, _, _, _, _ = env.step(0)\n",
    "ax = plt.subplot(121)\n",
    "plt.imshow(state)\n",
    "plt.title('Original image')\n",
    "plt.subplot(122)\n",
    "plt.imshow(image_preprocess(state), cmap='gray')\n",
    "plt.title('Preprocessed image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import imageio\n",
    "\n",
    "def create_video(agent, video_path, \n",
    "                 nr_frames = STATE_SIZE[0],\n",
    "                 render = False):\n",
    "    env = gym.make('PongDeterministic-v4', render_mode = 'rgb_array')\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    if render:\n",
    "        img_list = [env.render()]\n",
    "    else:\n",
    "        img_list = [obs]\n",
    "    \n",
    "    state_buffer = deque(maxlen=nr_frames)\n",
    "    img = image_preprocess(obs)\n",
    "    while len(state_buffer) < nr_frames:\n",
    "        state_buffer.append(img)\n",
    "\n",
    "    state = np.asarray(state_buffer)\n",
    "\n",
    "    score = 0\n",
    "    for t in range(10000):\n",
    "        # TO DO: select an action\n",
    "        action = agent.act(state, 0)\n",
    "        next_obs, reward, done, _, _ = env.step(preprocess_action(action))\n",
    "        \n",
    "        if render:\n",
    "            img_list.append(env.render())\n",
    "        else:\n",
    "            img_list.append(next_obs)\n",
    "            \n",
    "        state_buffer.append(image_preprocess(next_obs))\n",
    "        state = np.asarray(state_buffer)\n",
    "\n",
    "        if done:\n",
    "            break \n",
    "    env.close()\n",
    "\n",
    "    # Create a video file using imageio\n",
    "    imageio.mimsave(video_path, img_list, fps=30)\n",
    "\n",
    "    print (f'Visualization done. Episode ended after {t} steps.')\n",
    "    \n",
    "    mp4 = open(video_path,'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    return \"\"\"\n",
    "    <video width=400 controls>\n",
    "        <source src=\"%s\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\" % data_url    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Implement a Dueling Neural Network\n",
    "Implement the neural network (`QNetwork` class):\n",
    "1) In `__init__()` Create a neural network\n",
    "2) In `forward()` implement the forward pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class DuelingCNN(nn.Module):\n",
    "    \"\"\"Neural network that estimates Q values.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Args:\n",
    "            state_size (int, int, int): Size of the state (nr_chanels, x, y).\n",
    "            action_size (int): Dimension of each action. It equals \n",
    "                the number of the network outputs\n",
    "\n",
    "        \"\"\"\n",
    "        super(DuelingCNN, self).__init__()\n",
    "        \n",
    "        nr_chanels, x, y = state_size\n",
    "        \n",
    "        # TODO: Create convolutional layers as follows:\n",
    "        #     Conv2d: 16 3x3 filters, padding='same', activation=ReLU)\n",
    "        #     MaxPool2d: 2x2 kernel\n",
    "        #     Conv2d: 32 3x3 filters, padding='same', activation=ReLU)\n",
    "        #     MaxPool2d: 2x2 kernel\n",
    "        # Note - if you use nn.Sequential, you just separate the layers by commas\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # ENTER YOUR CODE HERE\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Pass an input tensor through the convolutional block\n",
    "        # to calculate the number of neurons in the flatten layer\n",
    "        input_tensor = torch.randn(1, nr_chanels, x, y)\n",
    "        output = self.conv_layers(input_tensor)\n",
    "        self.nr_flatten = np.prod(output.size()[1:])\n",
    "\n",
    "        # TODO: Create value and advantage layers as follows:\n",
    "        #     Flatten\n",
    "        #     Dense: 128 units, activation=ReLU\n",
    "        #     Dense: 1 unit (value)\n",
    "\n",
    "        self.value_block = nn.Sequential(\n",
    "            # ENTER YOUR CODE HERE\n",
    "        )\n",
    "\n",
    "        # TODO: Create value and advantage layers as follows:\n",
    "        #     Flatten\n",
    "        #     Dense: 128 units, activation=ReLU\n",
    "        #     Dense: asction_size units (advantage)\n",
    "        \n",
    "        self.advantage_block = nn.Sequential(\n",
    "            # ENTER YOUR CODE HERE\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\n",
    "        Args:\n",
    "            state (torch.Tensor): The state of the environment\n",
    "        Returns:\n",
    "            torch.Tensor: The action values\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: Calculate features from conv_layers\n",
    "        features = None # ENTER YOUR CODE HERE\n",
    "        \n",
    "        # Flatten the features\n",
    "        features = features.view(features.size(0), -1)\n",
    "\n",
    "        # TODO: Calculate advantage and value from features\n",
    "        value = None # ENTER YOUR CODE HERE\n",
    "        advantage = None # ENTER YOUR CODE HERE\n",
    "        \n",
    "        # TODO: Undestand the following line\n",
    "        q = value + advantage - advantage.mean(dim=-1, keepdim=True)\n",
    "        return q\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Implement the Agent:\n",
    "Implement the `Agent` class.\n",
    "1) In `__init__()` create a neural network, optimizer and Replay Buffer\n",
    "2) In `step()` add the experience to the memory buffer\n",
    "3) In `learn()` calculate `next_actions`, `Q_targets`, and `loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size,\n",
    "                 buffer_size = int(1e4),\n",
    "                 batch_size = 64,\n",
    "                 gamma = 0.99,\n",
    "                 lr = 5e-4,\n",
    "                 update_every = 4,\n",
    "                 device = None,\n",
    "                 load_from = None):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        Args:\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): replay buffer size. Default: int(1e5)\n",
    "            batch_size (int): minibatch size. Default: 64\n",
    "            gamma (float): discount factor. Default: 0.99\n",
    "            lr (float): learning rate. Default: 5e-4\n",
    "            update_every (int): how often to update the network. Default: 1        \n",
    "            device (torch.device): device to use. Default: None    \n",
    "            load_from (str): path to the checkpoint file to load. Default: None\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.update_every = update_every\n",
    "        self.device = device\n",
    "\n",
    "        # Q-Network\n",
    "        fc1, fc2 = 64, 16 # Size of the layers\n",
    "        # TODO: Create local and target networks with 2 hidden layers (fc1 and fc2 nodes)\n",
    "        # Remember about .to(device)\n",
    "        self.local_network = None # ENTER YOUR CODE HERE\n",
    "        self.target_network = None # ENTER YOUR CODE HERE\n",
    "        self.target_hard_update()       \n",
    "        \n",
    "        # TODO: Create the optimizer (Adam with learning rate lr)\n",
    "        self.optimizer = None # ENTER YOUR CODE HERE\n",
    "\n",
    "        # TODO: Create a Replay Buffer. Use ReplayBuffer class from the \n",
    "        # reply_buffer.py file. Use buffer_size as the buffer size\n",
    "        self.memory = None # ENTER YOUR CODE HERE\n",
    "        \n",
    "        # Initialize time step (for updating every self.update_every steps)\n",
    "        self.t_step = 0\n",
    "        \n",
    "        if load_from is not None:\n",
    "            self.load(load_from)\n",
    "            \n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # TODO: Save experience in replay buffer. Use self.memory.add()\n",
    "        # ENTER YOUR CODE HERE\n",
    "        \n",
    "        # Learn every self.update_every time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample(self.batch_size)\n",
    "                self.learn(experiences)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.local_network.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.local_network(state)\n",
    "        self.local_network.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            action =  np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            action = random.choice(np.arange(self.action_size))\n",
    "        return action\n",
    "        #return preprocess_action(action)\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        device = self.device  # for shortening the following lines\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.LongTensor(actions.reshape(-1, 1)).to(device)\n",
    "        rewards = torch.FloatTensor(rewards.reshape(-1, 1)).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones = torch.FloatTensor(dones.reshape(-1, 1)).to(device)        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # TODO: Calculate next actions using local network\n",
    "            # Hint1: You can use argmax(1)\n",
    "            # Hint2: Remember to use detach() to avoid backpropagation\n",
    "            # Hint3: You need to keep the dimentions (use keepdim)\n",
    "            next_actions = None # ENTER YOUR CODE HERE\n",
    "            \n",
    "            # Get max predicted Q values (for next states) from target model\n",
    "            Q_next = self.target_network(next_states).gather(1, next_actions)\n",
    "            \n",
    "            # TODO: Compute Q targets for current states \n",
    "            Q_targets = None # ENTER YOUR CODE HERE\n",
    "\n",
    "        # Get expected Q values \n",
    "        Q_curr = self.local_network(states).gather(1, actions)\n",
    "\n",
    "        # TODO: Compute loss. Use F.mse_loss\n",
    "        loss = None # ENTER YOUR CODE HERE\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def target_hard_update(self):\n",
    "        \"\"\" Assigns Local network weights to target network weights\n",
    "        \"\"\"\n",
    "        self.target_network.load_state_dict(self.local_network.state_dict())\n",
    "        self.target_network.eval()        \n",
    "\n",
    "        \n",
    "    def save(self, path = 'checkpoint.pth'):\n",
    "        \"\"\"Saves the network\n",
    "        Args:\n",
    "            path (str): path to save the network. Default is 'checkpoint.pth'\n",
    "        \"\"\"\n",
    "        torch.save(self.local_network.state_dict(), path)\n",
    " \n",
    "    def load(self, path = 'checkpoint.pth'):\n",
    "        \"\"\"Loads the network\n",
    "        Args:\n",
    "            path (str): path with the weights. Default is 'checkpoint.pth'\n",
    "        \"\"\"\n",
    "        self.local_network.load_state_dict(torch.load(path))\n",
    "        self.target_hard_update()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained agent\n",
    "Training an agent from scratch is doable, but time-consuming. On Google Colab (with GPU) it takes at least 1 hour. As an alternative you may use a pretrained agent - an agent trained for 300 episodes. This agent is still far from being perfect, yet it already knows basics, which speeds up the training process.\n",
    "\n",
    "To use the pretrained agent set `LOAD_PRETRAINED = True`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PRETRAINED = True\n",
    "video_url = None\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    state_size=STATE_SIZE, \n",
    "    #action_size=env.action_space.n, \n",
    "    action_size = ACTION_SIZE,\n",
    "    device=device)\n",
    "\n",
    "if LOAD_PRETRAINED:\n",
    "    agent.load('Notebook02_steps300.pth')\n",
    "    create_video(agent, 'output.mp4')\n",
    "    video_url = create_video(agent, 'output.mp4')\n",
    "\n",
    "HTML(video_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Train the Agent\n",
    "Complete the `train_agent()` function.\n",
    "1) Select and perform an action\n",
    "2) Use `agent.step()` to update the agent's knowledge\n",
    "3) Decay the explorarion rate epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(\n",
    "        n_episodes = 10000,        # Maximum number of episodes\n",
    "        max_t = 10000,             # Maximum number of timesteps per episode\n",
    "        eps_start = 1.0,           # Starting value of epsilon\n",
    "        eps_end = 0.01,            # Minimum value of epsilon\n",
    "        eps_decay = 0.99,          # Decay rate of epsilon\n",
    "        expected_reward = 18,      # Expected average reward to solve the environment\n",
    "        update_target_every = 1,   # Update the target network every x steps        \n",
    "        nr_frames = STATE_SIZE[0], # Number of frames to stack\n",
    "        ):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    Args: \n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for \n",
    "            epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): decay factor (per episode) \n",
    "            for decreasing epsilon\n",
    "        expected_reward (float): finish when the average score\n",
    "            is greater than this value\n",
    "        upate_target_every (int): how often should the target \n",
    "            network be updated. Default: 1 (per every episode)  \n",
    "        nr_frames (int): number of frames to stack           \n",
    "    Returns:\n",
    "        scores (list): list of scores from each episode        \n",
    "    \"\"\"\n",
    "    scores = []                        \n",
    "    scores_window = deque(maxlen=100)  \n",
    "    eps = eps_start                    \n",
    "    \n",
    "    state_buffer = deque(maxlen=nr_frames)\n",
    "    \n",
    "    step_nr = 0\n",
    "\n",
    "    if LOAD_PRETRAINED:\n",
    "        episode_start = 300\n",
    "        eps = max(eps_end, eps * eps_decay ** (episode_start-1))\n",
    "    else:\n",
    "        episode_start = 1\n",
    "        \n",
    "    for episode in range(episode_start, n_episodes+1):\n",
    "        obs, info = env.reset()\n",
    "        \n",
    "        img = image_preprocess(obs)\n",
    "        while len(state_buffer) < nr_frames:\n",
    "            state_buffer.append(img)\n",
    "\n",
    "        state = np.asarray(state_buffer)\n",
    "        \n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            # TO DO: select an action\n",
    "            action = None # ENTER YOUR CODE HERE\n",
    "            \n",
    "            # Note that we are preprocessing the action to make it compatible with the environment\n",
    "            next_obs, reward, done, truncated, info = env.step(preprocess_action(action))\n",
    "            \n",
    "            state_buffer.append(image_preprocess(next_obs))\n",
    "            next_state = np.asarray(state_buffer)\n",
    "            \n",
    "            # TO DO: use agent.step() to update the agent\n",
    "            # ENTER YOUR CODE HERE\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       \n",
    "        scores.append(score)\n",
    "        \n",
    "        # TO DO: Decay epsilon by multiplying it by eps_decay             \n",
    "        # ENTER YOUR CODE HERE \n",
    "        \n",
    "        step_nr = (step_nr + 1) % update_target_every\n",
    "        if episode % update_target_every == 0:\n",
    "            agent.target_hard_update()     \n",
    "        \n",
    "        mean_score = np.mean(scores_window)\n",
    "        \n",
    "        print(f'\\rEpisode {episode}, \\tsteps: {t}, \\tAverage Score: {mean_score:.2f}     ', end=\"\")\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(f'\\rEpisode {episode}, \\tsteps: {t}, \\tAverage Score: {mean_score:.2f}')\n",
    "            agent.save(f'checkpoint{str(episode).zfill(4)}.pth')\n",
    "        if mean_score >= expected_reward:\n",
    "            print(f'\\nDone in {episode:d} episodes!\\tAverage Score: {mean_score:.2f}')\n",
    "            agent.save(f'checkpoint{str(episode).zfill(4)}.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "t_start = time()\n",
    "scores = train_agent()\n",
    "t_end = time()\n",
    "print(f'Training done in {t_end - t_start:.1f} seconds')\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #') \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the agent's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_video(agent, 'output.mp4')\n",
    "video_url = create_video(agent, 'output.mp4')\n",
    "\n",
    "HTML(video_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
