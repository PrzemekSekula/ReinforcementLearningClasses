{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "---\n",
    "In this notebook you are going to implement the `REINFORCE` algorithm.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PrzemekSekula/ReinforcementLearningClasses/blob/master/REINFORCE/REINFORCE.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pseudocode (general case):\n",
    "\n",
    "1. Use the policy $\\pi_w$ to collect $m$ trajectories $\\tau^{(1)}, \\tau^{(2)}, ..., \\tau^{(m)}$ with horizon $H$. We refer to the $i$-th trajectory as:\n",
    "$$\\tau^{(i)} = (s_0^{(i)}, a_0^{(i)}, r_1^{(i)}, s_1^{(i)}, a_1^{(i)}, r_2^{(i)}, ...., s_H^{(i)}, a_H^{(i)}, r_H^{(i)} s_{H+1}^{(i)})$$\n",
    "\n",
    "2. Use the trajectories to estimate the gradient $\\nabla_wU(w)$:\n",
    "$$\\nabla_wU(w) \\approx \\hat{g} := \\frac{1}{m} \\sum_{i=1}^m \\sum_{t=0}^H \\nabla \\log{\\pi(a_t^{(i)} | s_t^{(i)})} * R(\\tau^{(i)}) $$ \n",
    "\n",
    "where $R(\\tau^{(i)})=\\sum_{t=1}^H \\gamma^{t-1}*r_t^{(i)}$ is a sum of all (discounted) rewards collected during the trajectory $\\tau^{(i)}$.\n",
    "\n",
    "3. Update the weights of the policy:\n",
    "    $$w \\leftarrow w + \\alpha * \\hat{g}$$\n",
    "\n",
    "4. If the results are not satisfactory, go to step 1.\n",
    "\n",
    "#### Pseudocode (update policy with each trajectory)\n",
    "In our implementation we are going to simplify two aspects of this algorithm:\n",
    "- We are collecting only one trajectory and update the policy with it.\n",
    "- We assume that the trajectory always ends at terminal state (or after `max_t` steps if terminal state was not reached).\n",
    "\n",
    "With these asumptions the `REINFORCE` pseudocode looks as follows:\n",
    "\n",
    "1. Use the policy $\\pi_w$ to collect a trajectory:\n",
    "$$\\tau = (s_0, a_0, r_1, s_1, a_1, r_2, ...., s_{T-1}, a_{T-1}, r_T, s_{T})$$\n",
    "\n",
    "2. Use the collected trajectory to estimate the gradient $\\nabla_wU(w)$:\n",
    "$$\\nabla_wU(w) \\approx \\hat{g} := \\sum_{t=0}^H \\nabla \\log{\\pi(a_t | s_t)} * R(\\tau) $$ \n",
    "\n",
    "where $R(\\tau)=\\sum_{t=1}^T \\gamma^{t-1}*r_t$ is a sum of all (discounted) rewards collected during the trajectory $\\tau$.\n",
    "\n",
    "3. Update the weights of the policy:\n",
    "    $$w \\leftarrow w + \\alpha * \\hat{g}$$\n",
    "\n",
    "4. If the results are not satisfactory, go to step 1.    \n",
    "\n",
    "#### Pseudocode - implementation details\n",
    "It is possible to simplify things even more during implementation. Please note, that while we are collecting a trajectory, we are using a policy (neural network) to sample the action. This requires computing the probabilities of each action. Instead of collecting the trajectory and then reusing this network to compute this probability again, we may simply collect the probability (or even the log probability) and the corresponding rewards. With this assumption, the first step of the algorithm looks as follows:\n",
    "\n",
    "1. Using policy $\\pi_w$ generate the episode. Collect the log probabilities $\\log{\\pi(a_t | s_t)}$ and rewards $r_t$. At the end of the episode you should have two list\n",
    "$$LogProbabilities = (\\log{\\pi(a_0 | s_0)}, \\log{\\pi(a_1 | s_1)}, ..., \\log{\\pi(a_{T-1} | s_{T-1})})$$\n",
    "$$Rewards = (r_1, r_2, ..., r_T)$$\n",
    "\n",
    "Steps 2, 3, and 4 are the same, yet as we collected all the log probabilities, we do not have to use the policy to calculate them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install --upgrade gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipython_show_video(path: str) -> None:\n",
    "    from IPython.display import HTML, display\n",
    "    \"\"\"Show a video at `path` within IPython Notebook.\"\"\"\n",
    "    if not os.path.isfile(path):\n",
    "        raise NameError(\"Cannot access: {}\".format(path))\n",
    "\n",
    "    video = io.open(path, \"r+b\").read()\n",
    "    encoded = base64.b64encode(video)\n",
    "\n",
    "    display(HTML(\n",
    "        data=\"\"\"\n",
    "        <video width=\"320\" height=\"240\" alt=\"test\" controls>\n",
    "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"/>\n",
    "        </video>\n",
    "        \"\"\".format(encoded.decode(\"ascii\"))\n",
    "    ))\n",
    "\n",
    "\n",
    "def show_latest_video(video_folder: str) -> str:\n",
    "    \"\"\"Show the most recently recorded video from video folder.\"\"\"\n",
    "    list_of_files = glob.glob(os.path.join(video_folder, \"*.mp4\"))\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    ipython_show_video(latest_file)\n",
    "    return latest_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Policy\n",
    "Create a policy (neural network) that you are going to train. Fill in the `__init()`, `forward` and `act` methods. \n",
    "- For the network use the parameters as follows:\n",
    "    - Input layer: 4 neurons (size of the state vector)\n",
    "    - Hidden layer: 16 neurons, relu activation function\n",
    "    - Output layer: 2 neurons (number of actions), `softmax` activation function\n",
    "- In `act` method:\n",
    "    - compute probabilities (use `forward()`) \n",
    "    - sample action proportionally to probabilities (use `m.sample()`)\n",
    "    - return: \n",
    "        - action (use `action.item()` to change tensor to a value) \n",
    "        - logarithm of probability of the selected action (use `m.log_prob(action`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size=4, h_size=16, a_size=2):\n",
    "        super(Policy, self).__init__()\n",
    "        # Create neural network layers here\n",
    "        # ENTER YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement forward pass\n",
    "        # ENTER YOUR CODE HERE\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = None # ENTER YOUR CODE HERE (computer probabilities of each action)\n",
    "        m = Categorical(probs)\n",
    "        action = None # ENTER YOUR CODE HERE (sample an action from the distribution)\n",
    "        return None, None # ENTER YOUR CODE HERE (return action and log_prob(action))\n",
    "\n",
    "\n",
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Generate episode.\n",
    "Your goal is to generate the episode and store both logarithmic probabilities of each selected action and corresponding rewards. You are going to need these probabilities and rewards to implement the `REINFORCE` algorithm. Fill in the `generate_episode` function. You should:\n",
    "- determine the initial state by reseting the environment\n",
    "- use `policy` to get both the next action and the probabilities of all actions\n",
    "- perform the selected action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(policy, max_t=1000):\n",
    "    saved_log_probs = []\n",
    "    rewards = []\n",
    "    \n",
    "    state, _ = None # ENTER YOUR CODE HERE (reset the environment and get the initial state)\n",
    "    for t in range(max_t):\n",
    "        action, log_prob = None # ENTER YOUR CODE HERE (select an action)\n",
    "        state, reward, done, _, _ = None # ENTER YOUR CODE HERE (perform a step in the environment)\n",
    "        \n",
    "        saved_log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    return saved_log_probs, rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Update policy\n",
    "- Use `generate_episode()` to obtain the logarithms of probabilities of selected actions and the rewards that correspond to these actions.\n",
    "- Discount the rewards. This is not necessary if you use `gamma=1.0` yet better to have it in your code.\n",
    "- Calculate the sum of discounted rewards $R(\\tau)$.\n",
    "- For each action calculate a corresponding loss as $loss_{elem} = -log{\\pi(a|s)} * R(\\tau)$ where:\n",
    "    - $log{\\pi(a|s)}$ is the logarithm of the probability of the selected action (you already have it)\n",
    "    - $R(\\tau)$ is a sum of all the discounted rewards (you should calculate it in the previous point)\n",
    "\n",
    "*Note: Remember to use the **negative** log probability $-log{\\pi(a|s)}$. `optimizer.step()` uses **Gradient Descent** algorithm and you want to use **Gradient Ascent** to maximize the expected rewards. Thus, you should use negative log probabilities to get minus gradients with `policy_loss.backwards()`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(n_episodes=2000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        saved_log_probs, rewards = None # ENTER YOUR CODE HERE (generate an episode)\n",
    "        \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))        \n",
    "        \n",
    "        R_discounted = None # ENTER YOUR CODE HERE (compute discounted rewards)\n",
    " \n",
    "        R = None # ENTER YOUR CODE HERE (compute cumulative rewards)\n",
    "        \n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(None) # ENTER YOUR CODE HERE (compute policy loss)\n",
    "            \n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print(f'Episode {i_episode}\\tAverage Score: {np.mean(scores_deque):.2f}')\n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print(f'Environment solved in {i_episode} episodes!\\tAverage Score: {np.mean(scores_deque):.2f}')\n",
    "            break\n",
    "        \n",
    "    return scores\n",
    "    \n",
    "scores = reinforce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "plot_scores(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our policy behaves (it does not work in Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_actions(policy):\n",
    "    env = gym.make('CartPole-v1', render_mode = 'rgb_array')\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    img_list = [env.render()]\n",
    "\n",
    "    for t in range(1000):\n",
    "        action, _ = policy.act(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        img_list.append(env.render())\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "    env.close()\n",
    "\n",
    "    plt.ion()\n",
    "    animation = plt.imshow(img_list[0])\n",
    "    for img in img_list:\n",
    "        animation.set_data(img) \n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "\n",
    "    print (f'Visualization done. Episode ended after {t} steps.')\n",
    "\n",
    "visualize_actions(policy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Assignment Demo (Optional)\n",
    "\n",
    "Modify your algorithm to use only the future rewards. Calculate $\\hat{g}$ as:\n",
    "\n",
    "$$\\hat{g} := \\sum_{t=0}^T R_t^{future} \\nabla \\log{\\pi(a_t | s_t)} $$\n",
    "\n",
    "where:\n",
    "$$R_t^{future}=\\sum_{i=t}^Tr_i$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
