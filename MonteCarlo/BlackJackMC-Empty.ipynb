{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Black Jack with Monte Carlo\n",
    "In this task you are asked to find an optimal policy for a Black Jack game. You are going to use an OpenAI Gym [Black Jack environment](https://gym.openai.com/envs/Blackjack-v0/) in this task. [OpenAI Gym](https://gym.openai.com/) is a toolkit for developing and comparing reinforcement learning algorithms. One of its features is to provide various RL-ready environments to facilitate studing and developing new Reinforcement Learning algorithms.\n",
    "\n",
    "The main purposes of this notebook are to introduce:\n",
    "- OpenAI Gym environments\n",
    "- Monte Calro Methods\n",
    "- the `Exploring starts` exploration algorithm \n",
    "\n",
    "\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PrzemekSekula/ReinforcementLearningClasses/blob/main/MonteCarlo/BlackJackMC-Empty.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym\n",
    "#!pip install pygame\n",
    "\n",
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install --upgrade gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Black Jack Environment\n",
    "- `states` - are provided as tuples (`score`, `dealer score`, `useable ace`)\n",
    "    - `score` - the summary score of your cards (4-21)\n",
    "    - `dealer score` - the first dealer card (1-10)\n",
    "    - `useable ace` - True/False, points out if you have a useable ace\n",
    "- `actions`\n",
    "    - `0` - draw\n",
    "    - `1` - hit\n",
    "- `rewards`\n",
    "    - `1` - you won a game\n",
    "    - `0` - draw\n",
    "    - `-1` - you lost a game\n",
    "    \n",
    "    \n",
    "Let's create an environment and see how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Fill in the placeholders to complete the `update_qest` method.  You are supposed\n",
    "to compute an updated state-action value according to the formula:\n",
    "\n",
    "\n",
    "$q_{n+1} = q_{n} + \\frac{1}{n}(G_n-q_n)$\n",
    "\n",
    "where:\n",
    "- $q_{n}$ - current estimated state-action value \n",
    "- $q_{n+1}$ - new estimated state-action value \n",
    "- $G_n$ - return obtained for the explored state and action\n",
    "- $n$ - number of actions (computed separately for each action type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    \"\"\"\n",
    "    This class is used to learn and maintain the policy. It is being done by\n",
    "    learning action-value methods. \n",
    "    Properties: \n",
    "        q_est - a dictionary that stores estimated state-action values for \n",
    "                the states\n",
    "                { state : [value for action0, value for action1] }\n",
    "        n     - a dictionary that stores how many times each state-action \n",
    "                value was updated\n",
    "                { state : [no. updates for action0, no. updates for action 1]}\n",
    "    Methods:\n",
    "        act         - returns a greedy action according to a current policy.\n",
    "                      The initial policy assumes to hit until the score >= 19.\n",
    "                      Then it is gradually updated in a learning process\n",
    "        update_qest - updates a specific state-action value using a given \n",
    "                      return\n",
    "        plot        - visualizes the policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Dictionary with { state : [state action value for 0, state action value for 1]  }\n",
    "        self.q_est = {}        \n",
    "        self.n = {}\n",
    "        self.__initialize_states()\n",
    "        \n",
    "    def __initialize_states(self):\n",
    "        \"\"\"\n",
    "        Initializes all possible states by seting q_est and n values\n",
    "        - q_est is set to hit if score < 19, otherwise it is set to draw\n",
    "          (the value of the preferred action is set to 1, the value of the \n",
    "          other action is set to 0)\n",
    "        - n is set to 0 for both actions\n",
    "        \"\"\"\n",
    "        for score in range(4, 22):\n",
    "            for dealer_score in range(1, 11):\n",
    "                for usable_ace in [True, False]:\n",
    "                    should_hit = int (score < 19)\n",
    "                    state = (score, dealer_score, usable_ace)\n",
    "                    self.q_est[state] = [1 - should_hit, should_hit]\n",
    "                    self.n[state] = [0, 0]\n",
    "        \n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Returns a greedy action according to the current policy\n",
    "        Arguments:\n",
    "            state - a state obtained from OpenAI Gym Black Jack environment\n",
    "        Returns:\n",
    "            action - 0 for 'draw', 1 for 'hit'\n",
    "        \"\"\"\n",
    "        return np.argmax(self.q_est[state])\n",
    "                \n",
    "    def update_qest(self, state, action, g):\n",
    "        \"\"\"\n",
    "        Updates state-action value for a specific state and a specific action.\n",
    "        State-action values are computed as a mean of all returns\n",
    "        Arguments:\n",
    "            state  - a state obtained from OpenAI Gym Black Jack environment\n",
    "            action - 0 for 'draw', 1 for 'hit'\n",
    "            g      - return that should be used for updating        \n",
    "        \"\"\"\n",
    "        \n",
    "        # ENTER YOUR CODE HERE. \n",
    "        # Update self.n[state][action] and self.q_est[state][action]\n",
    "\n",
    "        \n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "    def plot(self, useable_ace = True):\n",
    "        \"\"\"\n",
    "        Plots a visualization of current policy. It plots the policy only for the explored\n",
    "        states. The states that haven't been explored yet, are plotted as 'unknown'\n",
    "        Arguments:\n",
    "            useable_ace - True / False. It plots different policies, whether the player\n",
    "                          has or does not have a useable ace.        \n",
    "        \"\"\"\n",
    "        states = [x for x in self.q_est.keys() if x[2] == useable_ace]\n",
    "        rows = max(x[0] for x in states)\n",
    "        cols = max(x[1] for x in states)\n",
    "        \n",
    "        res = -1 * np.ones((rows, cols))\n",
    "        for state in states:\n",
    "            res[rows-state[0], state[1]-1] = self.act(state)\n",
    "            \n",
    "        fig = plt.figure(figsize = (12, 8))\n",
    "\n",
    "        #cbar_kws = {'ticks' : [0, 0.5, 1]}\n",
    "        cmap = matplotlib.colors.ListedColormap(('white', 'r', 'g'), name = 'My Cmap')\n",
    "\n",
    "        ax = sns.heatmap(res, linewidth=0.5, cmap = cmap)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        cbar.set_ticks([1, 0, -1])\n",
    "        cbar.set_ticklabels(['hit', 'draw', 'unknown'])\n",
    "        ax.set_xticks(np.arange(10) + 1)\n",
    "        ticks = np.arange(10)\n",
    "        xticklabels = [f'       {x}' for x in list(ticks+1)]\n",
    "\n",
    "        plt.xticks(ticks, xticklabels, ha = 'left')\n",
    "        ticks = np.arange(21)\n",
    "        yticklabels = [f'{x} ' + (' ' if x < 10 else '') for x in list(ticks+1)[::-1]]\n",
    "        plt.yticks(ticks, yticklabels, rotation=90, va='top')\n",
    "        plt.title('Black Jack policy with' + ('' if useable_ace else 'out') + ' a useable ace')\n",
    "\n",
    "        plt.show()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "policy.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "Complete the generate_episode function by filling in the code placeholders\n",
    "You are supposed to:\n",
    "- randomly select the initial action and execute this action \n",
    "- select every other action according to the policy and execute it\n",
    "- update policy for each state-action pair generated during the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(policy, env):\n",
    "    \"\"\"\n",
    "    Generates one episode, and updates state-action values of the policy\n",
    "    acording to the given policy.\n",
    "    Argumets:\n",
    "        policy - a policy object\n",
    "        env    - an OpenAI Gym Black Jack environment\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    \n",
    "    state, info = env.reset()\n",
    "    \n",
    "    # ENTER YOUR CODE HERE\n",
    "    # Choose the first action randomly\n",
    "    action = None\n",
    "    # END OF YOUR CODE\n",
    "    \n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "    \n",
    "    # ENTER YOUR CODE HERE\n",
    "    # Execute the action you chose above\n",
    "    state, reward, terminal, truncated, info = None\n",
    "    # END OF YOUR CODE\n",
    "    \n",
    "    while not terminal:\n",
    "        # ENTER YOUR CODE HERE\n",
    "        # Choose an action according to the current policy\n",
    "        action = None\n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "                \n",
    "        # ENTER YOUR CODE HERE\n",
    "        # Execute the action you chose above\n",
    "        state, reward, terminal, truncated, info = None\n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "    for state, action in zip(states[::-1], actions[::-1]):\n",
    "        # ENTER YOUR CODE HERE\n",
    "        # Update qest     \n",
    "        pass \n",
    "        # END OF YOUR CODE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "Generate episodes and learn from them until you learn the optimal policy. You may use `policy.plot()` to visualize your policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER YOUR CODE HERE\n",
    "# Generate enough episodes to obtain the optimal policy\n",
    "for i in tqdm(range (10)):\n",
    "    generate_episode(policy, env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.plot(useable_ace = False)\n",
    "policy.plot(useable_ace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
