{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarsa & Q Learning\n",
    "In this exercise you are going to implement two algorithms: `SARSA` and `Q-Learning`.\n",
    "Both `SARSA` and `Q-Learning` are examples of `Temporal Diference` (TD) algorithms. The main diference is in update rule. `SARSA` is an On-policy method - we are computing $TD_{target}$ using the same policy that we used for explorarion ($\\epsilon-Greedy$): \n",
    "\n",
    "$Q(S_t, A_t) = Q(S_t, A_t) + \\alpha[R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$\n",
    "\n",
    "whereas in `Q-Learning` we are using $\\epsilon-Greedy$ policy to explore, and $Greedy$ policy to compute $TD_{target}$:\n",
    "\n",
    "$Q(S_t, A_t) = Q(S_t, A_t) + \\alpha[R_{t+1} + \\gamma \\max\\limits_{a'} Q(S_{t+1}, a') - Q(S_t, A_t)$\n",
    "\n",
    "In the first two task you are going to work with the [Cliff Walking](https://www.gymlibrary.dev/environments/toy_text/cliff_walking/) environment, whereas in task 3 the [Taxi](https://www.gymlibrary.dev/environments/toy_text/taxi/) enviornment will be used.\n",
    "\n",
    "\n",
    "### Tasks Overview\n",
    "1. Task 1: SARSA and [Cliff Walking](https://www.gymlibrary.dev/environments/toy_text/cliff_walking/) environment.\n",
    "\n",
    "2. Task 2: Q-Learning and [Cliff Walking](https://www.gymlibrary.dev/environments/toy_text/cliff_walking/) environment\n",
    "    - In `agents.py` complete the `QLearningAgent` class (you must compliete `get_action` and `update` methods)\n",
    "    - set the parameters $\\alpha$, $\\gamma$, and $\\epsilon$ for the `qAgent`\n",
    "    - Complete the learning loop in this notebook. Run it and train the agent.\n",
    "    - Test the agent. You may test it by using `python render.py -a QLearning` from the command line. Make sure that your agent chooses actions correctly.\n",
    "3. Task 3. [Taxi](https://www.gymlibrary.dev/environments/toy_text/taxi/) enviornment\n",
    "    - Apply both `SARSA` and `QLearning` agents for the [Taxi](https://www.gymlibrary.dev/environments/toy_text/taxi/) enviornment. Test the results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from agents import SarsaAgent, QLearningAgent\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cliff walking Environment.\n",
    "![alt text](https://www.gymlibrary.dev/_images/cliff_walking.gif)\n",
    "\n",
    "### Task 1 - SARSA\n",
    "1. In `agents.py` complete the `SarsaAgent` class (you must complete `get_action` and `update` methods).\n",
    "2. Set the parameters $\\alpha$, $\\gamma$, and $\\epsilon$ for the `sarsaAgent`.\n",
    "3. Complete the learning loop in this notebook. Run it and train the agent.\n",
    "5. Test the agent. You may test it by using `python render.py -a SARSA` from the command line. Make sure that your agent chooses actions correctly. Change the parameters from point 2 or increase the number of training episodes if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CliffWalking-v0')\n",
    "# YOUR CODE HERE\n",
    "# Set the parameters of the agent\n",
    "sarsaAgent = SarsaAgent(env, alpha=0, gamma=0, epsilon=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_Rewards = []\n",
    "# YOUR CODE HERE\n",
    "# Change the number of episodes, if necessary\n",
    "for episode in tqdm(range(100)):\n",
    "    episode_reward = 0\n",
    "    state, info = env.reset()\n",
    "    \n",
    "    # Use the agent to choose the first action\n",
    "    action = None # YOUR CODE HERE\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Perform the selected action\n",
    "        next_state, reward, done, truncated, info = None # YOUR CODE HERE\n",
    "        # Select next action        \n",
    "        next_action = None # YOUR CODE HERE\n",
    "        # Update the agent\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Assign the next state and action to the current ones\n",
    "        state = None # YOUR CODE HERE\n",
    "        action = None # YOUR CODE HERE\n",
    "        episode_reward += reward\n",
    "    sarsa_Rewards.append(episode_reward)\n",
    "\n",
    "sarsaAgent.save()\n",
    "plt.plot(sarsa_Rewards)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Q-Learning\n",
    "1. In `agents.py` complete the `QLearningAgent` class (you must complete `get_action` and `update` methods)\n",
    "2. Set the parameters $\\alpha$, $\\gamma$, and $\\epsilon$ for the `qAgent`\n",
    "3. Complete the learning loop in this notebook. Run it and train the agent.\n",
    "4. Test the agent. You may test it by using `python render.py -a QLearning` from the command line. Make sure that your agent chooses actions correctly. Change the parameters from point 2 or increase the number of training episodes if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Set the parameters of the agent\n",
    "qAgent = QLearningAgent(env, alpha=0.1, gamma=0.95, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_rewards = []\n",
    "for episode in tqdm(range(100)):\n",
    "    episode_reward = 0\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Choose an action\n",
    "        action = None # YOUR CODE HERE\n",
    "        \n",
    "        # Perform the selected action\n",
    "        next_state, reward, done, truncated, info = None # YOUR CODE HERE\n",
    "\n",
    "        # Update the agent\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Assign the next state to the current one\n",
    "        state = None # YOUR CODE HERE\n",
    "        \n",
    "        episode_reward += reward\n",
    "    Q_rewards.append(episode_reward)\n",
    "        \n",
    "qAgent.save()\n",
    "plt.plot(Q_rewards)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Environment\n",
    "![alt text](https://www.gymlibrary.dev/_images/taxi.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Task 3 - Taxi environment\n",
    "- Apply both `SARSA` and `QLearning` agents for the [Taxi](https://www.gymlibrary.dev/environments/toy_text/taxi/) enviornment. Note that you may want to save the $Q_{estimated}$ values in the different files. You can do it using a parameter in `agent.save` method, for example: `sarsaAgent.save('Taxi_Sarsa.npy')` and `QAgent.save('Taxi_Q.npy')`\n",
    "- Test the agents. If you saved the parameters as above, you may test them using `python render.py -a SARSA -e Taxi-v3 -f Taxi_Sarsa.npy` for SARSA and `python render.py -a QLearning -e Taxi-v3 -f Taxi_Q.npy` for Q-Learning. Make sure that your agent chooses actions correctly. Change the agent parameters the number of training episodes if necessary.\n",
    "\n",
    "\n",
    "*Note: I noticed that the environment sometimes does not render correctly (rendering is not displayed despite the correct actions taken). If you observe something like this double-check the printed states - it may be simply the environment error.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
