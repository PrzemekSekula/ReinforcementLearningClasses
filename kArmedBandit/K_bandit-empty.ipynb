{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-armed bandit\n",
    "This notebook implements k-armed bandit and four agents:\n",
    "- Random agent\n",
    "- $\\epsilon$-greedy agent\n",
    "- Optimistic agent\n",
    "- Upper confidence bound agent\n",
    "\n",
    "The purpose of the notebook is to get you a good understanding of:\n",
    "- K-armed bandit problems\n",
    "- Exploration vs exploitation tradeoff\n",
    "- $\\epsilon$-greedy action selection method\n",
    "- Optimistic action selection method\n",
    "- Upper confidence bound action selection method\n",
    "\n",
    "## This is a graded assignment\n",
    "To pass the assinment, you are expected to:\n",
    "- perform all the code tasks, namely implement optimistic and ucb agents\n",
    "- understand the topics behind this code, that were covered in a lecture\n",
    "- present your code to the teacher and answer the questions\n",
    "\n",
    "### Related reading from the Reinforcement Learning book\n",
    "- Chapter 2-2.7 (Pages 25-36)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bandit class\n",
    "This class implements a k-armed bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    pass\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bandit object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = Bandit([4, 5, 3, 3, 1, 2, 0, 1, 5, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random agent\n",
    "Agent that acts randomly\n",
    "\n",
    "Parameters:\n",
    "- `q_est` - estimated $q_{\\star}$ values for each arm\n",
    "- `n` - how many times each arm was selected\n",
    "\n",
    "Methods:\n",
    "- `act(self, bandit)` - performs an action (selects an arm randomly) and updates coresponding $Q$ and $n$ values. $Q$ is updated according to the formula:\n",
    "\n",
    "$Q_{n+1} = Q{n} + \\frac{1}{n}(R_n-Q_n)$\n",
    "\n",
    "where:\n",
    "- $Q_{n}$ - current estimated $q_{\\star}$ value\n",
    "- $Q_{n+1}$ - new estimated $q_{\\star}$ value\n",
    "- $R_n$ - reward obtained for an action $n$\n",
    "- $n$ - number of actions (computed separately for each action type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the agent and see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards (rewards):\n",
    "    \"\"\"\n",
    "    Plots the rewards\n",
    "    \"\"\"\n",
    "    f = plt.figure()\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('reward')\n",
    "    plt.show()\n",
    "    \n",
    "plot_rewards(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_agent(agent, bandit):\n",
    "    \"\"\"\n",
    "    Displays the agent data, namely:\n",
    "    q_star - real expected rewards from the bandit\n",
    "    q_est  - expected rewards extimated by the agent\n",
    "    n      - how many times the agent selected this particular arm\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    df['q_star'] = bandit.q_star\n",
    "    df['q_est'] = agent.q_est\n",
    "    df['n'] = agent.n\n",
    "    return df\n",
    "\n",
    "display_agent(agent, bandit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-greedy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the agent n times (1000 steps each time) to see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display how the $\\epsilon$-greedy agent works.\n",
    "\n",
    "*Note: We trained many agents, and the displayed reward is the average of the rewards obrained by all the agents. However, the dataframe contains only the data for the latest agent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(rewards)\n",
    "display_agent(agent, bandit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may save the rewards for future comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rewards = pd.DataFrame()\n",
    "df_rewards['eGreedy'] = rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1  - Optimistic agent\n",
    "Please fill in the placeholders to implement the optimistic agent.\n",
    "Generally, you are suppose to prepate the `__init__` and `act` methods.\n",
    "- In the `__init__` method you should set the initial values optimistically.\n",
    "- In the `act` method you should:\n",
    "    - Perform an action (select the arm greedily based on the estimated $q_{\\star}$ values) \n",
    "    - Update the corresponding estimated $q_{\\star}$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimisticAgent:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimistic agent results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rewards['optimistic'] = rewards # Let's store the results for comparison\n",
    "plot_rewards(rewards)\n",
    "display_agent(agent, bandit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2  - Upper Confidence Bound agent\n",
    "\n",
    "Please fill in the placeholders to implement the UCB agent.\n",
    "Generally, you are suppose to prepate the `act` method. In this method you are supposed to:\n",
    "- Compute the optimistic estimated $q_{\\star}$ values for each action, according to the formula:\n",
    "$Q_{optimistic}(a) = Q_{estimated}(a) + c \\sqrt{\\frac{ln(t)}{N_t(a)}}$ where:\n",
    "    - $a$ - selected action\n",
    "    - $Q_{estimated}(a)$ - estimated $q_{\\star}$ value for action $a$\n",
    "    - $c$ - explorarion parameter\n",
    "    - $t$ - number of all steps taken so far\n",
    "    - $N_t(a)$ - number of times when action $a$ was selected\n",
    "- Perform an action (select the arm greedily based on the estimated $Q_{optimistic}$ values) \n",
    "- Update the corresponding estimated $q_{\\star}$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBAgent:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rewards['UCB'] = rewards # Let's store the results for comparison\n",
    "plot_rewards(rewards)\n",
    "display_agent(agent, bandit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - understanding the code\n",
    "\n",
    "See the results of all agents as well as the comparison below. Try to understand the results. To get the better understanding you may experiment with the code. For example, you may:\n",
    "- Modify the k-armed bandit by changeing the real $Q_{\\star}$ values and/or the standard deviation.\n",
    "- Change the $\\epsilon$ parameter in $\\epsilon$-greedy agent and see how it impacts the behaviour of the agent\n",
    "- Change the initial values for the optimistic agent and see how it impacts the behaviour of the agent\n",
    "- Change exploration parameter $c$ for the UCB agent and see how it impacts the behaviour of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rewards.plot(colormap = 'brg', ylim=[3, 5.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
