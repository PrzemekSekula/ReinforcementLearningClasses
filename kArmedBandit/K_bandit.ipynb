{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-armed bandit\n",
    "This notebook implements k-armed bandit and four agents:\n",
    "- Random agent\n",
    "- $\\epsilon$-greedy agent\n",
    "- Optimistic agent\n",
    "- Upper confidence bound agent\n",
    "\n",
    "The purpose of the notebook is to get you a good understanding of:\n",
    "- K-armed bandit problems\n",
    "- Exploration vs exploitation tradeoff\n",
    "- $\\epsilon$-greedy action selection method\n",
    "- Optimistic action selection method\n",
    "- Upper confidence bound action selection method\n",
    "\n",
    "## This is a graded assignment\n",
    "To pass the assinment, you are expected to:\n",
    "- perform all the code tasks, namely implement optimistic and ucb agents\n",
    "- understand the topics behind this code, that were covered in a lecture\n",
    "- present your code to the teacher and answer the questions\n",
    "\n",
    "### Related reading from the Reinforcement Learning book\n",
    "- Chapter 2-2.7 (Pages 25-36)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PrzemekSekula/ReinfocementLearningClasses/blob/master/kArmedBandit/K_bandit.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bandit class\n",
    "This class implements a k-armed bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    def __init__(self, q_star, stddev = 2):  \n",
    "        \"\"\"\n",
    "            Arguments:\n",
    "                k      - number of arms\n",
    "                minq   - minimum value of q_star for each arm\n",
    "                maxq   - maximum value of q_star for each arm\n",
    "                stddev - standard deviation that randomize the values while action is executed\n",
    "        \"\"\"\n",
    "        self.stddev = stddev\n",
    "        self.q_star = q_star\n",
    "                \n",
    "    def execute(self, arm):\n",
    "        \"\"\"\n",
    "        Returns the state of the bandit (randomly seleted value for a given arm)\n",
    "        Arguments:\n",
    "            arm - number of arm to be pulled\n",
    "        \"\"\"\n",
    "        return self.q_star[arm] + np.random.normal(scale=self.stddev)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bandit object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = Bandit([4, 5, 3, 3, 1, 2, 0, 1, 5, 1])\n",
    "print (bandit.q_star)\n",
    "bandit.execute(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random agent\n",
    "Agent that acts randomly\n",
    "\n",
    "Parameters:\n",
    "- `q_est` - estimated $q_{\\star}$ values for each arm\n",
    "- `n` - how many times each arm was selected\n",
    "\n",
    "Methods:\n",
    "- `act(self, bandit)` - performs an action (selects an arm randomly) and updates coresponding $Q$ and $n$ values. $Q$ is updated according to the formula:\n",
    "\n",
    "$Q_{n+1} = Q{n} + \\frac{1}{n}(R_n-Q_n)$\n",
    "\n",
    "where:\n",
    "- $Q_{n}$ - current estimated $q_{\\star}$ value\n",
    "- $Q_{n+1}$ - new estimated $q_{\\star}$ value\n",
    "- $R_n$ - reward obtained for an action $n$\n",
    "- $n$ - number of actions (computed separately for each action type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, bandit):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            bandit - bandit that the agent will operate on (used only to set initial values of q_est and n)\n",
    "        \"\"\"\n",
    "        self.q_est =  [0] * len(bandit.q_star)\n",
    "        self.n =  [0] * len(bandit.q_star)\n",
    "                \n",
    "    def act(self, bandit):\n",
    "        \"\"\"\n",
    "        Performs an action (selects an arm randomly) and updates corespondingn q_est and n values\n",
    "        Arguments:\n",
    "            bandit - bandit that the agent is operated on\n",
    "        Returns:\n",
    "            reward - reward from the bandit (a result of the performed action)\n",
    "        \"\"\"\n",
    "        arm = np.random.randint(len(self.q_est))\n",
    "        reward = bandit.execute(arm)\n",
    "        self.n[arm] += 1\n",
    "        \n",
    "        # Q_n+1 = Qn + (1 / n) * (R - Q_n)\n",
    "        self.q_est[arm] += (reward - self.q_est[arm]) / self.n[arm]\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the agent and see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RandomAgent(bandit)\n",
    "rewards = []\n",
    "for step in range (1000):\n",
    "    reward = agent.act(bandit)\n",
    "    rewards.append(reward)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards (rewards):\n",
    "    \"\"\"\n",
    "    Plots the rewards\n",
    "    \"\"\"\n",
    "    f = plt.figure()\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('reward')\n",
    "    plt.show()\n",
    "    \n",
    "plot_rewards(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_agent(agent, bandit):\n",
    "    \"\"\"\n",
    "    Displays the agent data, namely:\n",
    "    q_star - real expected rewards from the bandit\n",
    "    q_est  - expected rewards extimated by the agent\n",
    "    n      - how many times the agent selected this particular arm\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    df['q_star'] = bandit.q_star\n",
    "    df['q_est'] = agent.q_est\n",
    "    df['n'] = agent.n\n",
    "    return df\n",
    "\n",
    "display_agent(agent, bandit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-greedy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent:\n",
    "    def __init__(self, bandit, epsilon = 0.1):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            bandit - bandit that the agent will operate on (used only to set initial values of q_est and n)\n",
    "            epsilon - exploration parameter \n",
    "        \"\"\"        \n",
    "        self.q_est = [0] * len(bandit.q_star)\n",
    "        self.n =  [0] * len(bandit.q_star)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def act(self, bandit):\n",
    "        \"\"\"\n",
    "        Performs an action (uses epsilon-greedy approach to select an arm)\n",
    "        and pdates corespondingn q_est and n values\n",
    "        Arguments:\n",
    "            bandit - bandit that the agent is operated on\n",
    "        Returns:\n",
    "            reward - reward from the bandit (a result of the performed action)\n",
    "        \"\"\"        \n",
    "        if np.random.random() > self.epsilon:\n",
    "            arm = np.argmax(self.q_est)\n",
    "        else:\n",
    "            arm = np.random.randint(len(self.q_est))\n",
    "        reward = bandit.execute(arm)\n",
    "        self.n[arm] += 1\n",
    "        \n",
    "        # Q_n+1 = Qn + (1 / n) * (R - Q_n)\n",
    "        self.q_est[arm] += (reward - self.q_est[arm]) / self.n[arm]\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the agent n times (1000 steps each time) to see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = []\n",
    "for loop in tqdm(range (2000)):\n",
    "    agent = EpsilonGreedyAgent(bandit)\n",
    "    rewards = []\n",
    "    for step in range (1000):\n",
    "        reward = agent.act(bandit)\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    total_rewards.append(rewards)\n",
    "total_rewards = np.asarray(total_rewards)\n",
    "rewards = total_rewards.mean(axis=0)\n",
    "print (rewards.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display how the $\\epsilon$-greedy agent works.\n",
    "\n",
    "*Note: We trained many agents, and the displayed reward is the average of the rewards obrained by all the agents. However, the dataframe contains only the data for the latest agent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(rewards)\n",
    "display_agent(agent, bandit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may save the rewards for future comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rewards = pd.DataFrame()\n",
    "df_rewards['eGreedy'] = rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1  - Optimistic agent\n",
    "Please fill in the placeholders to implement the optimistic agent.\n",
    "Generally, you are suppose to prepate the `__init__` and `act` methods.\n",
    "- In the `__init__` method you should set the initial values optimistically.\n",
    "- In the `act` method you should:\n",
    "    - Perform an action (select the arm greedily based on the estimated $q_{\\star}$ values) \n",
    "    - Update the corresponding estimated $q_{\\star}$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimisticAgent:\n",
    "    def __init__(self, bandit, initial_value = 10):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            bandit - bandit that the agent will operate on (used only to set initial values of q_est and n)\n",
    "            initial_value - initial value for estimated q_star. It should be high enough to make the algorithm work\n",
    "        \"\"\"\n",
    "        \n",
    "        self.q_est =  None #Enter your code here\n",
    "        self.n =  [0] * len(bandit.q_star)\n",
    "                \n",
    "    def act(self, bandit):\n",
    "        \"\"\"\n",
    "        Performs an action (selects an arm greedily based on the estimated q_star values ) \n",
    "        and updates corespondingn q_est and n values\n",
    "        Arguments:\n",
    "            bandit - bandit that the agent is operated on\n",
    "        Returns:\n",
    "            reward - reward from the bandit (a result of the performed action)\n",
    "        \"\"\"\n",
    "        \n",
    "        arm = None # Enter your code here\n",
    "        reward = None # Enter your code here\n",
    "        self.n[arm] += 1\n",
    "        \n",
    "        # Q_n+1 = Qn + (1 / n) * (R - Q_n)\n",
    "        self.q_est[arm] = None # Enter your code here\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = []\n",
    "for loop in tqdm(range (2000)):\n",
    "    agent = OptimisticAgent(bandit)\n",
    "    rewards = []\n",
    "    for step in range (1000):\n",
    "        reward = agent.act(bandit)\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    total_rewards.append(rewards)\n",
    "total_rewards = np.asarray(total_rewards)\n",
    "rewards = total_rewards.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimistic agent results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rewards['optimistic'] = rewards # Let's store the results for comparison\n",
    "plot_rewards(rewards)\n",
    "display_agent(agent, bandit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2  - Upper Confidence Bound agent\n",
    "\n",
    "Please fill in the placeholders to implement the UCB agent.\n",
    "Generally, you are suppose to prepate the `act` method. In this method you are supposed to:\n",
    "- Compute the optimistic estimated $q_{\\star}$ values for each action, according to the formula:\n",
    "$Q_{optimistic}(a) = Q_{estimated}(a) + c \\sqrt{\\frac{ln(t)}{N_t(a)}}$ where:\n",
    "    - $a$ - selected action\n",
    "    - $Q_{estimated}(a)$ - estimated $q_{\\star}$ value for action $a$\n",
    "    - $c$ - explorarion parameter\n",
    "    - $t$ - number of all steps taken so far\n",
    "    - $N_t(a)$ - number of times when action $a$ was selected\n",
    "- Perform an action (select the arm greedily based on the estimated $Q_{optimistic}$ values) \n",
    "- Update the corresponding estimated $q_{\\star}$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBAgent:\n",
    "    def __init__(self, bandit, c = 3):\n",
    "        self.q_est =  [0] * len(bandit.q_star)\n",
    "        self.n =  [0] * len(bandit.q_star)\n",
    "        self.c = c\n",
    "        \n",
    "    def act(self, bandit): \n",
    "        # Calculate Q optimistic here\n",
    "        # Enter your code here (probably you need more than one line of code)\n",
    "        \n",
    "        arm = None # Enter your code Here\n",
    "        reward = None # Enter your code here\n",
    "        self.n[arm] += 1\n",
    "        \n",
    "        # Q_n+1 = Qn + (1 / n) * (R - Q_n)\n",
    "        self.q_est[arm] = None # Enter your code here\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = []\n",
    "for loop in tqdm(range (2000)):\n",
    "    agent = UCBAgent(bandit)\n",
    "    rewards = []\n",
    "    for i in range (1000):\n",
    "        reward = agent.act(bandit)\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    total_rewards.append(rewards)\n",
    "total_rewards = np.asarray(total_rewards)\n",
    "rewards = total_rewards.mean(axis=0)\n",
    "print (rewards.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rewards['UCB'] = rewards # Let's store the results for comparison\n",
    "plot_rewards(rewards)\n",
    "display_agent(agent, bandit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - understanding the code\n",
    "\n",
    "See the results of all agents as well as the comparison below. Try to understand the results. To get the better understanding you may experiment with the code. For example, you may:\n",
    "- Modify the k-armed bandit by changeing the real $Q_{\\star}$ values and/or the standard deviation.\n",
    "- Change the $\\epsilon$ parameter in $\\epsilon$-greedy agent and see how it impacts the behaviour of the agent\n",
    "- Change the initial values for the optimistic agent and see how it impacts the behaviour of the agent\n",
    "- Change exploration parameter $c$ for the UCB agent and see how it impacts the behaviour of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rewards.plot(colormap = 'brg', ylim=[3, 5.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
