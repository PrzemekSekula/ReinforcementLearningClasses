{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Deep Q-Learning \n",
    "---\n",
    "In this notebook, you will implement Dynamic Programming algorithm.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PrzemekSekula/ReinforcementLearningClasses/blob/main/DynamicProgramming/DP_task.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !wget https://raw.githubusercontent.com/PrzemekSekula/ReinforcementLearningClasses/main/DynamicProgramming/helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from helper import State, Simulator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World description\n",
    "### Actions\n",
    "Our world is a grid world. An agent can move the world travelling in four main directions, so four actions are possible:\n",
    "- 0 (up)\n",
    "- 1 (right)\n",
    "- 2 (down)\n",
    "- 3 (left)\n",
    "\n",
    "Each action moves us to the corresponding state. If it is impossible to be moved, we are staying in the same state. There is also a terminal state (marked red). Reaching a terminal state ends the episode.\n",
    "\n",
    "### States\n",
    "A state is a location of the agent. A standard way to describe the state is by using an instance of a `State` class. Such an object has two properties: `state.row` and `state.col` that are describing the position of the agent. Both rows and cols are counted from `0`, so the upper left corner corresponds to `(0, 0)` state. Another way of describing a state is by using a tuple `(row, col)`. Such a format is also accepted by methods implemented in a simulator.\n",
    "\n",
    "### Rewards\n",
    "For each move a negative reward `Reward = -1` is granted. Aditionally, for entering each state a reward associated with this state is granted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulator Description\n",
    "A main goals of a simulator are as follows:\n",
    "- store the data about the world\n",
    "- store the current policy\n",
    "- store the current value function\n",
    "- facilitate RL-related operations\n",
    "\n",
    "#### Properties:\n",
    "- `world` - numpy.array with the world. The numbers correspond to the rewards for reaching each state\n",
    "- `policy` - numpy.array with policy. Policy is always deterministic. The numbers represent specific actions: 0 (up), 1 (right), 2 (down), 3 (left)\n",
    "- `values` - numpy.array with the state-value function for each state\n",
    "- `reward` - aditional reward granted for performing each action\n",
    "- `terminal` - a terminal state. It is an instance of the `State` class. \n",
    "\n",
    "#### Methods:\n",
    "- `move` - Returns a state that is the result of an action.\n",
    "- `getReward` - Returns the reward for entering a specific state (location).\n",
    "- `getValue` - Returns a Value function for a determined state (location).\n",
    "- `getPolicy` - Returns a policy for a determined state (location).\n",
    "- `setValue` - Sets a Value function for a determined state (location).\n",
    "- `setPolicy` - Sets a policy for a determined state (location).\n",
    "- `plot` - Visualizes the world, value function and policy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = np.array([\n",
    "    [ 0,    0,   0,  0],\n",
    "    [-10, -10, -15,  0],\n",
    "    [ 0,   0,   0,   0],\n",
    "    [ 0, -10, -10, -10],\n",
    "    [ 0,   0,   0,   0],\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "sim = Simulator(\n",
    "    world = world, # Our World\n",
    "    terminal = [x-1 for x in world.shape], # t. state in lower right corner\n",
    "    reward = -1 # Reward for each step\n",
    "    )\n",
    "\n",
    "sim.policy = 3 + sim.world * 0\n",
    "sim.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward, Value, Policy\n",
    "We can communicate with world, reading Rewards, Value functions and \n",
    "Let's see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start with state (location) 1, 2 (row nr 1, column nr 2)\n",
    "\n",
    "def displayStateData(sim, state):\n",
    "    print (f'Reward: {sim.getReward(state)}')\n",
    "    print (f'Value: {sim.getValue(state)}')\n",
    "    print (f'Policy: {sim.getPolicy(state)}')\n",
    "\n",
    "displayStateData(sim, (1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting values and policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.setValue((1, 2), -1)\n",
    "sim.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.setPolicy((1, 2), 2)\n",
    "sim.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `State` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = State((1, 2))\n",
    "\n",
    "print (f'State: {state}')\n",
    "print (f'Row: {state.row}')\n",
    "print (f'Col: {state.col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayStateData(sim, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.setPolicy(state, 0)\n",
    "sim.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `simulator.move()` method\n",
    "There are two ways of using `simulator.move()` method. In the first version, you provide two arguments:\n",
    "- `state` - the starting location that you want to move from. It may be either an instance of a `State` class or a tupple with (row, col) coordinates\n",
    "- `action` - action to be performed. One of: 0 (up), 1 (right), 2 (down), 3 (left)\n",
    "\n",
    "The method returns a destination state (an instance of the `State()` class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (sim.move(state, 1)) # move right from (1, 2) point\n",
    "print (sim.move((0, 0), 3)) # Move left from (0, 0) point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second version you should just provide a starting location (state) that you want to move from. The action is selected from the current policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (sim.move(state)) # move from (1, 2) point, according to the policy (up)\n",
    "print (sim.move((3, 0))) # Move from (3, 0) point, according to the policy (left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "Create a code that uses one of DP algorithms to find an optimal policy for the given world.\n",
    "To pass the assignment you have to:\n",
    "- show that your code works\n",
    "- understand the code\n",
    "- understand the theory behind the code\n",
    "- understand the general idea of dynamic programming.\n",
    "\n",
    "*Note 1: It is not necessary to use the simulator, but you should at least consider it. It will make your life much easier.*\n",
    "\n",
    "*Note 2: You may use any discount rate you wish, but I recommend you to discount your rewards (use something < 1, eg. 0.9)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = -np.array([\n",
    "    [ 0,  0,  0,  0,  0,  0],\n",
    "    [10, 10, 11, 10, 10,  0],\n",
    "    [ 0,  0,  0,  0,  0,  0],\n",
    "    [ 0, 10, 10,  4, 10, 10],\n",
    "    [ 0,  0,  0,  0,  0,  0],\n",
    "    [10, 10, 10, 10, 10,  0],    \n",
    "    [ 0,  0,  0,  0,  0,  0],\n",
    "    [ 0, 10, 10, 10, 10, 10],\n",
    "    [ 0,  0, 0,   0,  0,  0],\n",
    "])\n",
    "\n",
    "\n",
    "sim = Simulator(\n",
    "    world = world, # Our World\n",
    "    terminal = [x-1 for x in world.shape], # t. state in lower right corner\n",
    "    reward = -1 # Reward for each step\n",
    "    )\n",
    "\n",
    "sim.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
